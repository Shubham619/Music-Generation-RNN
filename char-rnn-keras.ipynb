{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dropout, TimeDistributed, Dense, Activation, Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file=open(data_directory+data_file,\"r+\")\n",
    "str_input=file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "unq_chars={j:i for (i,j) in enumerate(sorted([i for i in set(str_input)]))}\n",
    "index_to_chars={j:i for (i,j) in unq_chars.items()}\n",
    "\n",
    "vocab_size=len(index_to_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=str_input\n",
    "length=len(T) #1,29992\n",
    "batch_chars=length/batch_size #(16)\n",
    "batch_chars=int(round(batch_chars,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory for data \n",
    "data_directory = \"Data/\"\n",
    "#includes input.txt which includes abc-notation music (ABC version of the nottingham music database (jigs (340 tunes)))\n",
    "data_file = \"input.txt\"\n",
    "#index file to storedistinct vocab \n",
    "charIndex_json = \"char_to_index.json\"\n",
    "#Saving model weights\n",
    "model_weights_directory = 'Data/Model_Weights/'\n",
    "#batch size\n",
    "batch_size = 16\n",
    "#sequence length\n",
    "seq_length = 64\n",
    "MODEL_DIR='./model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(epoch,model):\n",
    "    if not os.path.exists(MODEL_DIR):\n",
    "        os.makedirs(MODEL_DIR)\n",
    "    model.save_weights(os.path.join(MODEL_DIR,'weights.{}.h5'.format(epoch)))\n",
    "\n",
    "def load_weights(epoch,model):\n",
    "    model.load_weights(os.path.join(MODEL_DIR,'weights.{}.h5'.format(epoch)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_batches(inp,vocab_size):\n",
    "    for i in range(0,batch_chars-seq_length,seq_length):\n",
    "        X=np.zeros((batch_size,seq_length))\n",
    "        Y=np.zeros((batch_size,seq_length,vocab_size))\n",
    "        for j in range(0,batch_size):\n",
    "            for k in range(0,seq_length):\n",
    "                X[j,k]=inp[batch_chars*j+i+k]\n",
    "                Y[j,k,inp[batch_chars*j+i+k+1]]=1\n",
    "        yield X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (16, 64, 512)             25600     \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (16, 64, 256)             787456    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (16, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (16, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (16, 64, 50)              12850     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (16, 64, 50)              0         \n",
      "=================================================================\n",
      "Total params: 1,876,530\n",
      "Trainable params: 1,876,530\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model(batch_size, seq_length, unq_chars):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Embedding(input_dim = unq_chars, output_dim = 512, batch_input_shape = (batch_size, seq_length))) \n",
    "    \n",
    "    for i in range(3):\n",
    "        model.add(LSTM(256, return_sequences = True, stateful = True))\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(TimeDistributed(Dense(unique_chars)))\n",
    "\n",
    "    model.add(Activation(\"softmax\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "if __name__=='__main__':\n",
    "    model=build_model(16,64,50)\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(text, epochs = 1,save_fre=10):\n",
    "\n",
    "    \n",
    "    #model architecture\n",
    "    model = built_model(batch_size,seq_length, vocab_size)\n",
    "    model.summary()\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "    losses=[]\n",
    "    accs=[]\n",
    "    epoch_number=[]\n",
    "    print(losses)\n",
    "    #Train data generation\n",
    "    inp=np.asarray([unq_chars[c] for  c in text],dtype=np.int32)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
    "\n",
    "        for i, (X, Y) in enumerate(read_batches(inp, vocab_size)):\n",
    "\n",
    "            loss, acc = model.train_on_batch(X, Y) #check documentation of train_on_batch here: https://keras.io/models/sequential/\n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "            epoch_number.append(epoch+1)\n",
    "            print(\"Batch: {}, Loss: {}, Accuracy: {}\".format(i+1, loss, acc))\n",
    "            #here, above we are reading the batches one-by-one and train our model on each batch one-by-one.\n",
    "           \n",
    "       \n",
    "        #saving weights after every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            model.save_weights(os.path.join(model_weights_directory, \"Weights_{}.h5\".format(epoch+1)))\n",
    "   #creating dataframe and record all the losses and accuracies at each epoch\n",
    "    log_frame = pd.DataFrame(columns = [\"Epoch\", \"Loss\", \"Accuracy\"])\n",
    "    log_frame.loc[epoch,\"Epoch\"] = epoch+1\n",
    "    log_frame.loc[epoch,\"Loss\"] = np.average(losses)\n",
    "    log_frame.loc[epoch,\"Accuracy\"] = np.average(accs)\n",
    "    log_frame.to_csv(\"log.csv\", index = False)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_19 (Embedding)     (16, 64, 512)             44544     \n",
      "_________________________________________________________________\n",
      "lstm_55 (LSTM)               (16, 64, 256)             787456    \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_56 (LSTM)               (16, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "dropout_56 (Dropout)         (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "lstm_57 (LSTM)               (16, 64, 256)             525312    \n",
      "_________________________________________________________________\n",
      "dropout_57 (Dropout)         (16, 64, 256)             0         \n",
      "_________________________________________________________________\n",
      "time_distributed_19 (TimeDis (16, 64, 87)              22359     \n",
      "_________________________________________________________________\n",
      "activation_19 (Activation)   (16, 64, 87)              0         \n",
      "=================================================================\n",
      "Total params: 1,904,983\n",
      "Trainable params: 1,904,983\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "[]\n",
      "Epoch 1/1\n",
      "Batch: 1, Loss: 4.465936660766602, Accuracy: 0.009765625\n",
      "Batch: 2, Loss: 4.445473670959473, Accuracy: 0.1572265625\n",
      "Batch: 3, Loss: 4.400880813598633, Accuracy: 0.1337890625\n",
      "Batch: 4, Loss: 4.223415851593018, Accuracy: 0.1396484375\n",
      "Batch: 5, Loss: 3.786586284637451, Accuracy: 0.1669921875\n",
      "Batch: 6, Loss: 4.003255844116211, Accuracy: 0.1435546875\n",
      "Batch: 7, Loss: 3.6739702224731445, Accuracy: 0.1435546875\n",
      "Batch: 8, Loss: 3.668114423751831, Accuracy: 0.1015625\n",
      "Batch: 9, Loss: 3.7222683429718018, Accuracy: 0.080078125\n",
      "Batch: 10, Loss: 3.6163392066955566, Accuracy: 0.06640625\n",
      "Batch: 11, Loss: 3.600456953048706, Accuracy: 0.068359375\n",
      "Batch: 12, Loss: 3.5956454277038574, Accuracy: 0.056640625\n",
      "Batch: 13, Loss: 3.5000085830688477, Accuracy: 0.1103515625\n",
      "Batch: 14, Loss: 3.504544496536255, Accuracy: 0.1328125\n",
      "Batch: 15, Loss: 3.510535478591919, Accuracy: 0.1484375\n",
      "Batch: 16, Loss: 3.58897066116333, Accuracy: 0.14453125\n",
      "Batch: 17, Loss: 3.392524003982544, Accuracy: 0.1748046875\n",
      "Batch: 18, Loss: 3.208104372024536, Accuracy: 0.1826171875\n",
      "Batch: 19, Loss: 3.313971519470215, Accuracy: 0.1767578125\n",
      "Batch: 20, Loss: 3.512495756149292, Accuracy: 0.1376953125\n",
      "Batch: 21, Loss: 3.7741267681121826, Accuracy: 0.1083984375\n",
      "Batch: 22, Loss: 3.4322097301483154, Accuracy: 0.134765625\n",
      "Batch: 23, Loss: 3.279118299484253, Accuracy: 0.130859375\n",
      "Batch: 24, Loss: 3.2657079696655273, Accuracy: 0.1748046875\n",
      "Batch: 25, Loss: 3.345541477203369, Accuracy: 0.173828125\n",
      "Batch: 26, Loss: 3.661777973175049, Accuracy: 0.1337890625\n",
      "Batch: 27, Loss: 3.270458459854126, Accuracy: 0.1787109375\n",
      "Batch: 28, Loss: 3.1870198249816895, Accuracy: 0.1826171875\n",
      "Batch: 29, Loss: 3.310917377471924, Accuracy: 0.1748046875\n",
      "Batch: 30, Loss: 3.461790084838867, Accuracy: 0.150390625\n",
      "Batch: 31, Loss: 3.282804250717163, Accuracy: 0.15625\n",
      "Batch: 32, Loss: 3.2557313442230225, Accuracy: 0.1630859375\n",
      "Batch: 33, Loss: 3.2825374603271484, Accuracy: 0.16015625\n",
      "Batch: 34, Loss: 3.2996599674224854, Accuracy: 0.1552734375\n",
      "Batch: 35, Loss: 3.346329689025879, Accuracy: 0.1572265625\n",
      "Batch: 36, Loss: 3.2982006072998047, Accuracy: 0.1552734375\n",
      "Batch: 37, Loss: 3.363251209259033, Accuracy: 0.140625\n",
      "Batch: 38, Loss: 3.2372002601623535, Accuracy: 0.1416015625\n",
      "Batch: 39, Loss: 3.1798415184020996, Accuracy: 0.1572265625\n",
      "Batch: 40, Loss: 3.3572869300842285, Accuracy: 0.142578125\n",
      "Batch: 41, Loss: 3.3244588375091553, Accuracy: 0.1474609375\n",
      "Batch: 42, Loss: 3.0985496044158936, Accuracy: 0.1640625\n",
      "Batch: 43, Loss: 3.0730013847351074, Accuracy: 0.1884765625\n",
      "Batch: 44, Loss: 3.16705060005188, Accuracy: 0.1865234375\n",
      "Batch: 45, Loss: 3.2744765281677246, Accuracy: 0.150390625\n",
      "Batch: 46, Loss: 3.237102508544922, Accuracy: 0.158203125\n",
      "Batch: 47, Loss: 3.1320745944976807, Accuracy: 0.1865234375\n",
      "Batch: 48, Loss: 3.2306368350982666, Accuracy: 0.1650390625\n",
      "Batch: 49, Loss: 3.138507843017578, Accuracy: 0.1787109375\n",
      "Batch: 50, Loss: 3.265831708908081, Accuracy: 0.150390625\n",
      "Batch: 51, Loss: 3.1885464191436768, Accuracy: 0.162109375\n",
      "Batch: 52, Loss: 3.112626552581787, Accuracy: 0.1904296875\n",
      "Batch: 53, Loss: 3.141509532928467, Accuracy: 0.1630859375\n",
      "Batch: 54, Loss: 3.163254499435425, Accuracy: 0.1630859375\n",
      "Batch: 55, Loss: 3.225569248199463, Accuracy: 0.1484375\n",
      "Batch: 56, Loss: 3.285918712615967, Accuracy: 0.138671875\n",
      "Batch: 57, Loss: 3.1891331672668457, Accuracy: 0.1533203125\n",
      "Batch: 58, Loss: 3.154547691345215, Accuracy: 0.166015625\n",
      "Batch: 59, Loss: 3.195798397064209, Accuracy: 0.1611328125\n",
      "Batch: 60, Loss: 3.1053080558776855, Accuracy: 0.1640625\n",
      "Batch: 61, Loss: 3.248185157775879, Accuracy: 0.1455078125\n",
      "Batch: 62, Loss: 3.135774612426758, Accuracy: 0.162109375\n",
      "Batch: 63, Loss: 3.1820449829101562, Accuracy: 0.16796875\n",
      "Batch: 64, Loss: 3.1142807006835938, Accuracy: 0.1865234375\n",
      "Batch: 65, Loss: 3.1112332344055176, Accuracy: 0.1650390625\n",
      "Batch: 66, Loss: 3.1105806827545166, Accuracy: 0.1669921875\n",
      "Batch: 67, Loss: 3.213860034942627, Accuracy: 0.154296875\n",
      "Batch: 68, Loss: 3.2022438049316406, Accuracy: 0.1552734375\n",
      "Batch: 69, Loss: 3.158519744873047, Accuracy: 0.1611328125\n",
      "Batch: 70, Loss: 3.079042911529541, Accuracy: 0.1708984375\n",
      "Batch: 71, Loss: 3.101149082183838, Accuracy: 0.16796875\n",
      "Batch: 72, Loss: 3.2011234760284424, Accuracy: 0.173828125\n",
      "Batch: 73, Loss: 3.120866537094116, Accuracy: 0.17578125\n",
      "Batch: 74, Loss: 3.0547823905944824, Accuracy: 0.189453125\n",
      "Batch: 75, Loss: 3.0941052436828613, Accuracy: 0.17578125\n",
      "Batch: 76, Loss: 2.9962987899780273, Accuracy: 0.18359375\n",
      "Batch: 77, Loss: 3.068873882293701, Accuracy: 0.1728515625\n",
      "Batch: 78, Loss: 3.0334675312042236, Accuracy: 0.19140625\n",
      "Batch: 79, Loss: 3.010324478149414, Accuracy: 0.1884765625\n",
      "Batch: 80, Loss: 3.0181803703308105, Accuracy: 0.1875\n",
      "Batch: 81, Loss: 2.8799619674682617, Accuracy: 0.2041015625\n",
      "Batch: 82, Loss: 3.068653106689453, Accuracy: 0.1796875\n",
      "Batch: 83, Loss: 2.96812105178833, Accuracy: 0.1953125\n",
      "Batch: 84, Loss: 3.0385708808898926, Accuracy: 0.19140625\n",
      "Batch: 85, Loss: 3.0718116760253906, Accuracy: 0.193359375\n",
      "Batch: 86, Loss: 2.9094905853271484, Accuracy: 0.212890625\n",
      "Batch: 87, Loss: 3.048262119293213, Accuracy: 0.185546875\n",
      "Batch: 88, Loss: 3.019753932952881, Accuracy: 0.2197265625\n",
      "Batch: 89, Loss: 3.066803455352783, Accuracy: 0.1875\n",
      "Batch: 90, Loss: 3.0894596576690674, Accuracy: 0.193359375\n",
      "Batch: 91, Loss: 2.983156204223633, Accuracy: 0.2109375\n",
      "Batch: 92, Loss: 3.0002880096435547, Accuracy: 0.2001953125\n",
      "Batch: 93, Loss: 2.9992423057556152, Accuracy: 0.2001953125\n",
      "Batch: 94, Loss: 2.8642406463623047, Accuracy: 0.208984375\n",
      "Batch: 95, Loss: 2.9351537227630615, Accuracy: 0.2158203125\n",
      "Batch: 96, Loss: 2.876333713531494, Accuracy: 0.197265625\n",
      "Batch: 97, Loss: 2.8971357345581055, Accuracy: 0.2216796875\n",
      "Batch: 98, Loss: 2.8572263717651367, Accuracy: 0.2197265625\n",
      "Batch: 99, Loss: 2.877250909805298, Accuracy: 0.2060546875\n",
      "Batch: 100, Loss: 2.869462013244629, Accuracy: 0.23046875\n",
      "Batch: 101, Loss: 2.8520004749298096, Accuracy: 0.2197265625\n",
      "Batch: 102, Loss: 2.9878201484680176, Accuracy: 0.208984375\n",
      "Batch: 103, Loss: 2.906308174133301, Accuracy: 0.2109375\n",
      "Batch: 104, Loss: 2.853745460510254, Accuracy: 0.2158203125\n",
      "Batch: 105, Loss: 2.6836037635803223, Accuracy: 0.2431640625\n",
      "Batch: 106, Loss: 2.845722198486328, Accuracy: 0.2255859375\n",
      "Batch: 107, Loss: 2.727477788925171, Accuracy: 0.2578125\n",
      "Batch: 108, Loss: 2.863582134246826, Accuracy: 0.2060546875\n",
      "Batch: 109, Loss: 2.8018503189086914, Accuracy: 0.2529296875\n",
      "Batch: 110, Loss: 2.754706859588623, Accuracy: 0.2197265625\n",
      "Batch: 111, Loss: 2.7906126976013184, Accuracy: 0.216796875\n",
      "Batch: 112, Loss: 2.831453323364258, Accuracy: 0.2412109375\n",
      "Batch: 113, Loss: 2.787470817565918, Accuracy: 0.228515625\n",
      "Batch: 114, Loss: 2.8657543659210205, Accuracy: 0.216796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 115, Loss: 2.791325569152832, Accuracy: 0.2470703125\n",
      "Batch: 116, Loss: 2.8140859603881836, Accuracy: 0.2353515625\n",
      "Batch: 117, Loss: 2.8305163383483887, Accuracy: 0.2373046875\n",
      "Batch: 118, Loss: 2.7980198860168457, Accuracy: 0.23046875\n",
      "Batch: 119, Loss: 2.8383116722106934, Accuracy: 0.240234375\n",
      "Batch: 120, Loss: 2.7792344093322754, Accuracy: 0.2490234375\n",
      "Batch: 121, Loss: 2.6455259323120117, Accuracy: 0.283203125\n",
      "Batch: 122, Loss: 2.72941255569458, Accuracy: 0.2646484375\n",
      "Batch: 123, Loss: 2.7642486095428467, Accuracy: 0.2451171875\n",
      "Batch: 124, Loss: 2.686966896057129, Accuracy: 0.259765625\n",
      "Batch: 125, Loss: 2.655385971069336, Accuracy: 0.27734375\n",
      "Batch: 126, Loss: 2.668875217437744, Accuracy: 0.2861328125\n"
     ]
    }
   ],
   "source": [
    "k=train(str_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-df3c821d0997>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlosses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png',show_shapes=True, show_layer_names=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
